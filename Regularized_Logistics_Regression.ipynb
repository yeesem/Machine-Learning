{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBAWjKvrzyPBqWJQG1lj8+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeesem/Machine-Learning/blob/main/Regularized_Logistics_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "LUKjSUrVyJe-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I2tzAcSmxX_G"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "\n",
        "    g = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b, *argv):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    cost = 0\n",
        "#     for i in range(m):\n",
        "#       f = sigmoid(np.dot(X[i],w) + b)\n",
        "#       cost = -y[i]*np.log(f) - (1 - y[i])*np.log(1 - f)\n",
        "#       cost+= cost\n",
        "    f = sigmoid(np.dot(X,w) + b)\n",
        "    cost = np.sum(-y*np.log(f) - (1 - y)*np.log(1 - f))\n",
        "    total_cost = cost/m\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "anXhXmWxxlu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b, *argv):\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        z_wb = np.dot(X[i],w) + b\n",
        "        f_wb = sigmoid(z_wb)\n",
        "        for j in range(n):\n",
        "            dj_dw[j] += (f_wb - y[i]) * X[i,j]\n",
        "\n",
        "        dj_db_i = (f_wb - y[i])\n",
        "        dj_db += dj_db_i\n",
        "\n",
        "    #Alternative method\n",
        "#     f = sigmoid(np.dot(X,w)+b)\n",
        "#     dj_dw = np.dot((f - y), X)\n",
        "#     dj_db = np.sum(f-y)\n",
        "\n",
        "    dj_dw/=m\n",
        "    dj_db/=m\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "qzq9ILZpyUed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w_in = w_in - alpha * dj_dw\n",
        "        b_in = b_in - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
        "            w_history.append(w_in)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "\n",
        "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "3e54e42Rzg5k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    # Loop over each example\n",
        "#     for i in range(m):\n",
        "#         z_wb = 0\n",
        "#         # Loop over each feature\n",
        "#         for j in range(n):\n",
        "#             z_wb += X[i,j] * w[j]\n",
        "\n",
        "#         z_wb += b\n",
        "\n",
        "#         f_wb = sigmoid(z_wb)\n",
        "\n",
        "#         p[i] = True if f_wb>=0.5 else False\n",
        "\n",
        "    z_wb = sigmoid(np.dot(X,w)+b)\n",
        "    p = z_wb>=0.5\n",
        "    return p"
      ],
      "metadata": {
        "id": "B8-u29Lg0dGg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regularized Logistics Regression**"
      ],
      "metadata": {
        "id": "R0Bfnpcgzn6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    # Calls the compute_cost function that you implemented above\n",
        "    cost_without_reg = compute_cost(X, y, w, b)\n",
        "\n",
        "    # You need to calculate this value\n",
        "    reg_cost = 0.\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    reg_cost = lambda_/(2*m) * np.sum(w**2)\n",
        "\n",
        "    #Alternative method\n",
        "    # for j in range(n):\n",
        "    #   reg_cost = reg_cost + w[j]**2\n",
        "    #reg_cost = lambda_/(2*m) * reg_cost\n",
        "\n",
        "    # Add the regularization cost to get the total cost\n",
        "    total_cost = cost_without_reg + reg_cost\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "1s1TFKHbzk5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
        "\n",
        "#     for j in range(n):\n",
        "#         dj_dw[j] +=  (lambda_/m) * w[j]\n",
        "\n",
        "    dj_dw += lambda_/m * w\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "HsTvnQ1u0Rv-"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}